{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning HW01\n",
    "Gerald Baulig 0780827 2019/11/05\n",
    "GitHub: https://github.com/bugerry87/fcnn_scratch\n",
    "\n",
    "---\n",
    "\n",
    "In this assignment we are asked to implement a neural network from scratch,\n",
    "including Backpropagation and Storastic Gradient Decent algorithms.\n",
    "The assigment includes two tasks:\n",
    "\n",
    "1. Implement a regression network, and\n",
    "2. implement a classification network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement a Regression Network\n",
    "\n",
    "Implement a regression network for the engery efficiency dataset.\n",
    "I guess this dataset descripes the termal exchange of a building to its environment.\n",
    "The dataset has 2 Values (t):\n",
    "\n",
    "- Heating load, and\n",
    "- Cooling load\n",
    "\n",
    "And 8 features:\n",
    "\n",
    "- Relative compactness,\n",
    "- Surface area,\n",
    "- Wall area,\n",
    "- Roof area,\n",
    "- Overall hight,\n",
    "- Orientation,\n",
    "- Glazing area, and\n",
    "- Glazing area distribution.\n",
    "\n",
    "We are required to define a feature-vector out of these features and predict the heating load of the buildings, by minimizing the \"sum-of-squares\" error function,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E(w) = \\sum_{n=1}^{N}{(t_n - y(X_n;w))^2}\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "...while the evaluation should be processed by the \"root-mean-square\" error (basically an Euclidean distance :-),\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E_{RMS}(w) = \\frac{\\sqrt{\\sum_{n=1}^{N}{(t_n - y(x_n;w))^2}}}{\\sqrt{N}}\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement a Classifier\n",
    "\n",
    "Implement a classification network for the Ionosphere dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Fully-Connected Neural Network from Scratch\n",
    "\n",
    "First, let's define what a Neural Network is.\n",
    "A Neural Network is a class that can train and validate the weight of a stack of layers, w.r.t several hyper-parameters.\n",
    "However, all what the Neural Network needs to know about his own network is, where to get its output from. Furthermore, the hyper-parameters are only required in the training process, but only few of them in the validation process,\n",
    "such that the class defination could look like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, train_set, val_set, cost_func, loss_func):\n",
    "        self.train_set = train_set\n",
    "        self.val_set = val_set\n",
    "        self.cost = cost_func\n",
    "        self.loss = loss_func\n",
    "        pass\n",
    "    \n",
    "    def train(self, output_layer, lr, lr_dec):\n",
    "        self.lr = lr\n",
    "        self.lr_dec = lr_dec\n",
    "        self.training = True\n",
    "        for Y in output_layer.poll_forward():\n",
    "            Z = np.squeeze(self.cost.f(self.T, Y))\n",
    "            dZ = self.cost.d(self.T, Y)\n",
    "            output_layer.push_backward(dZ, self.lr)\n",
    "            self.lr -= self.lr * self.lr_dec\n",
    "            yield Z \n",
    "        pass\n",
    "    \n",
    "    def val(self, output_layer):\n",
    "        self.training = False\n",
    "        for Y in output_layer.poll_forward():\n",
    "            Z = self.loss.f(self.T, Y)\n",
    "            yield Z\n",
    "        pass        \n",
    "    \n",
    "    def gen_input(self):\n",
    "        if self.training:\n",
    "            for self.X, self.T, self.epoch, self.step in self.train_set():\n",
    "                yield self.X\n",
    "        else:\n",
    "            for self.X, self.T, _, _ in self.val_set():\n",
    "                yield self.X\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 5. 0. 0.]\n",
      "[1. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class ReLU:\n",
    "    f = lambda x: np.maximum(x,0).astype(float)\n",
    "    d = lambda x: (x > 0).astype(float)\n",
    "\n",
    "#test\n",
    "print(ReLU.f(np.array([1,-1,5,-5,0])))\n",
    "print(ReLU.d(np.array([1,-1,5,-5,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73105858 0.26894142 0.99330715 0.00669285 0.5       ]\n",
      "[0.19661193 0.19661193 0.00664806 0.00664806 0.25      ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class Sigmoid:\n",
    "    f = lambda x: 1 / (1 + np.exp(-x))\n",
    "    def d(x):\n",
    "        S = Sigmoid.f(x)\n",
    "        return S * (1 - S)\n",
    "    pass\n",
    "\n",
    "#test\n",
    "print(Sigmoid.f(np.array([1,-1,5,-5,0])))\n",
    "print(Sigmoid.d(np.array([1,-1,5,-5,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  5 -5  0]\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class Linear:\n",
    "    f = lambda x: x\n",
    "    d = lambda x: np.ones(x.shape)\n",
    "\n",
    "#test\n",
    "print(Linear.f(np.array([1,-1,5,-5,0])))\n",
    "print(Linear.d(np.array([1,-1,5,-5,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Define a Fully-Connected Layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FcLayer:\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 act_func,\n",
    "                 input_func,\n",
    "                 backprop,\n",
    "                 clipping=False\n",
    "                ):\n",
    "        self.w = params[0]\n",
    "        self.b = params[1]\n",
    "        self.act = act_func\n",
    "        self.input = input_func\n",
    "        self.backprop = backprop\n",
    "        self.clipping = clipping\n",
    "        pass\n",
    "    \n",
    "    def poll_forward(self):\n",
    "        '''Polls the output from the underlying layer. (Feed Forward)'''\n",
    "        for self.x in self.input():\n",
    "            self.y = self.act.f(np.dot(self.x, self.w) + self.b)\n",
    "            yield self.y\n",
    "    \n",
    "    def push_backward(self, dZ, lr):\n",
    "        '''Pushes the loss to the underlying layer. (Back Propagation)'''\n",
    "        dZ = dZ * self.act.d(self.y)\n",
    "        dw = np.dot(self.x.T, dZ)\n",
    "        if self.backprop:\n",
    "            self.backprop(np.dot(dZ, self.w.T), lr)\n",
    "        self.w += dw * lr if not self.clipping else np.clip(dw * lr, -1, 1)\n",
    "        if not self.b is 0:\n",
    "            self.b += np.sum(dZ, axis=0) * lr\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS.f 26\n",
      "SOS.d [ 0 -2  0 10  0]\n",
      "RMS.f 2.280350850198276\n",
      "RMS.d [ 0.  -0.2  0.   1.   0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class SOS:\n",
    "    f = lambda t,y: np.sum(np.power(t - y, 2))\n",
    "    d = lambda t,y: (t - y) * 2\n",
    "    \n",
    "class RMS:\n",
    "    f = lambda t,y: np.power(SOS.f(t, y) / t.size, 0.5)\n",
    "    d = lambda t,y: (SOS.d(t, y) / t.size) * 0.5\n",
    "\n",
    "#test\n",
    "print(\"SOS.f\", SOS.f(np.array([1,-2,5,-5,0]), np.array([1,-1,5,-10,0])))\n",
    "print(\"SOS.d\", SOS.d(np.array([1,-2,5,-5,0]), np.array([1,-1,5,-10,0])))\n",
    "print(\"RMS.f\", RMS.f(np.array([1,-2,5,-5,0]), np.array([1,-1,5,-10,0])))\n",
    "print(\"RMS.d\", RMS.d(np.array([1,-2,5,-5,0]), np.array([1,-1,5,-10,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression for Energy Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare Energy Efficiency Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.98 514.5  294.   110.25   0.     1.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.    15.55  21.33]\n",
      " [  0.98 514.5  294.   110.25   0.     0.     1.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.    15.55  21.33]\n",
      " [  0.98 514.5  294.   110.25   0.     0.     0.     1.     0.     0.\n",
      "    0.     0.     0.     0.     0.    15.55  21.33]\n",
      " [  0.98 514.5  294.   110.25   0.     0.     0.     0.     1.     0.\n",
      "    0.     0.     0.     0.     0.    15.55  21.33]\n",
      " [  0.9  563.5  318.5  122.5    0.     1.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.    20.84  28.28]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unpack_EnergyEfficiency_data():\n",
    "    raw = np.genfromtxt('EnergyEfficiency_data.csv', skip_header=1, delimiter=',')\n",
    "    N = raw.shape[0]\n",
    "    data = np.zeros((N,17))\n",
    "    data[:,:4] = raw[:,:4]     #comp, surf, wall, roof, \n",
    "    #data[:,4] = raw[:,4]       #hight\n",
    "    data[:,5] = raw[:,5] == 2  #ori north\n",
    "    data[:,6] = raw[:,5] == 3  #ori east\n",
    "    data[:,7] = raw[:,5] == 4  #ori south\n",
    "    data[:,8] = raw[:,5] == 5  #ori west\n",
    "    data[:,9] = raw[:,6]       #area\n",
    "    data[:,10] = raw[:,7] == 1 #area uniform\n",
    "    data[:,11] = raw[:,7] == 2 #area north\n",
    "    data[:,12] = raw[:,7] == 3 #area east\n",
    "    data[:,13] = raw[:,7] == 4 #area south\n",
    "    data[:,14] = raw[:,7] == 5 #area west\n",
    "    data[:,15:] = raw[:,8:]    #heat, cold\n",
    "    return data, N\n",
    "\n",
    "#fix the random seed for the reproducibility.\n",
    "#(another hyper-param)\n",
    "np.random.seed(15)\n",
    "\n",
    "def gen_data(data, epochs, batch_size, with_trails=False):\n",
    "    step = 0\n",
    "    N = data.shape[0]\n",
    "    X = np.zeros((batch_size, 15))\n",
    "    T = np.zeros((batch_size, 1))\n",
    "    for e in range(epochs):\n",
    "        for n in range(N):\n",
    "            i = int(step % batch_size)\n",
    "            X[i] = data[n,:15]\n",
    "            T[i] = data[n,-2]\n",
    "            step += 1\n",
    "            is_trail = not with_trails and n+batch_size >= N\n",
    "            if not step % batch_size:\n",
    "                yield X, T, e+is_trail, step\n",
    "        if with_trails and step % batch_size:\n",
    "            yield X[:i], T[:i], e+1, step\n",
    "\n",
    "def split_data(data, N):\n",
    "    shuffle = np.arange(N)\n",
    "    np.random.shuffle(shuffle)\n",
    "    data = data[shuffle]\n",
    "    split = int(N*0.7)\n",
    "    train = data[:split]\n",
    "    val = data[split:]\n",
    "    return train, val, shuffle\n",
    "\n",
    "data, N = unpack_EnergyEfficiency_data()\n",
    "train, val, shuffle = split_data(data, N)\n",
    "\n",
    "#test\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Network\n",
    "\n",
    "Now let's configure our network.\n",
    "Thanks to Pythons prototyping feature, we simply add each layer to our NeuralNetwork instance.\n",
    "For a more photonic initalization of the parameters we define a function `init_params`.\n",
    "With this function we define how many inputs our layer expects and how many outputs this layer will produce.\n",
    "The number of inputs must be equivalent to the number of nodes the previous layer has,\n",
    "while output it the number of nodes the current layer has.\n",
    "\n",
    "Furthermore, there are a lot of hyper-parameters in the parameter initialization.\n",
    "Few of them are weight-scaling, the use of bias and the initalization itself.\n",
    "In default we choose an unscaled natural distributed random generator with bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def init_params(inp, outp, scale=1, bias=True, init_func=np.random.randn):\n",
    "    weights = init_func(inp, outp)\n",
    "    while not np.all(weights):\n",
    "        malicious = weights==0.0\n",
    "        weights[malicious] = init_func(malicious.shape)\n",
    "    weights *= scale\n",
    "    return weights, np.zeros(outp) if bias else 0\n",
    "\n",
    "def init_network(epochs, batch_size):\n",
    "    nn = NeuralNetwork(\n",
    "        lambda: gen_data(train, epochs, batch_size),\n",
    "        lambda: gen_data(val, 1, 1),\n",
    "        SOS,\n",
    "        SOS\n",
    "    )\n",
    "\n",
    "    nn.In = FcLayer(init_params(15, 10, scale=2),\n",
    "                  Sigmoid, #act func\n",
    "                  nn.gen_input,\n",
    "                  None,\n",
    "                  clipping=True\n",
    "                 )\n",
    "\n",
    "    nn.Fc1 = FcLayer(init_params(10, 10, scale=2),\n",
    "                  Sigmoid,\n",
    "                  In.poll_forward,\n",
    "                  In.push_backward,\n",
    "                  clipping=True\n",
    "                 )\n",
    "\n",
    "    nn.Fc2 = FcLayer(init_params(10, 10, scale=2),\n",
    "                  Sigmoid,\n",
    "                  Fc1.poll_forward,\n",
    "                  Fc1.push_backward,\n",
    "                  clipping=True\n",
    "                 )\n",
    "\n",
    "    nn.Out = FcLayer(init_params(10, 1, scale=1),\n",
    "                  Linear,\n",
    "                  Fc2.poll_forward,\n",
    "                  Fc2.push_backward,\n",
    "                  clipping=True\n",
    "                 )\n",
    "    \n",
    "    # let's map the model and layers to a dictionery for easy access\n",
    "    Model = {\n",
    "        'In':(nn.In.w, nn.In.b),\n",
    "        'Fc1':(nn.Fc1.w, nn.Fc1.b),\n",
    "        'Fc2':(nn.Fc2.w, nn.Fc2.b),\n",
    "        'Out':(nn.Out.w, nn.Out.b)\n",
    "    }\n",
    "\n",
    "    Layers = {\n",
    "        'In':nn.In,\n",
    "        'Fc1':nn.Fc1,\n",
    "        'Fc2':nn.Fc2,\n",
    "        'Out':nn.Out\n",
    "    }\n",
    "    \n",
    "    return nn, Model, Layers\n",
    "\n",
    "#test\n",
    "nn, Model, Layers = init_network(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save(model, name):\n",
    "    with open(name, 'wb') as file:\n",
    "        pickle.dump(Model, file)\n",
    "    pass\n",
    "\n",
    "def load(layers, name):\n",
    "    with open(name, 'rb') as file:\n",
    "        obj = pickle.load(file)\n",
    "    for k,v in obj.items():\n",
    "        if k in layers:\n",
    "            layers[k].w = v[0]\n",
    "            layers[k].b = v[1]\n",
    "    pass\n",
    "\n",
    "#test\n",
    "save(Model, \"test.pkl\")\n",
    "load(Layers, \"test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-28e35394c8b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"checkpoint.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-144-28e35394c8b5>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m(nn, lr, lr_dec, chkpnt, skip)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mbest_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_dec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mcost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-135-33950a2c5254>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, output_layer, lr, lr_dec)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0moutput_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "#reset the model\n",
    "nn, Model, Layers = init_network(100, 100)\n",
    "\n",
    "def run_training(nn, lr, lr_dec, chkpnt=None, skip=0):\n",
    "    training = []\n",
    "    validation = []\n",
    "    epoch = 0\n",
    "    cost = 0\n",
    "    best_loss = None\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for Z in nn.train(nn.Out, lr, lr_dec):\n",
    "        cost += Z / train.shape[0]\n",
    "        if nn.epoch > epoch:\n",
    "            loss = 0\n",
    "            epoch = nn.epoch\n",
    "            \n",
    "            for Zv in nn.val(nn.Out):\n",
    "                loss += Zv / val.shape[0]\n",
    "                \n",
    "            if chkpnt and (epoch > skip) and (best is None or loss < best):\n",
    "                best_loss = loss\n",
    "                best_epoch = epoch\n",
    "                save(Model, chkpnt)\n",
    "                \n",
    "            validation.append(loss)\n",
    "            training.append(cost)\n",
    "            cost = 0\n",
    "    return training, validation, best_epoch, best_loss\n",
    "\n",
    "training, validation, best_epoch, best_loss = run_training(nn, 1e-5, 1e-5, \"checkpoint.pkl\", skip=200)\n",
    "ax = plt.figure().subplots(1)\n",
    "ax.plot(range(nn.epoch), training, label='train')\n",
    "ax.plot(range(nn.epoch), validation, ':', label='val')\n",
    "ax.scatter(best_epoch, best_loss)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.title(\"Learning curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Best Epoch:\", best_epoch, \"with a Loss of:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(Layers, \"checkpoint.pkl\")\n",
    "\n",
    "nn.val_set = lambda: gen_data(data, 1, 1)\n",
    "target = []\n",
    "pred = []\n",
    "for Zv in nn.val(Out):\n",
    "    target.append(nn.T[0][0])\n",
    "    pred.append(Out.y[0][0])\n",
    "\n",
    "plt.plot(range(N), pred, label='prediction')\n",
    "plt.plot(range(N), target, label='target', linestyle=':')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "class NeuralNetwork2:\n",
    "    def __init__(self, lr):\n",
    "        self.weights1   = np.random.rand(15,10) \n",
    "        self.weights2   = np.random.rand(10,10)     \n",
    "        self.weights3   = np.random.rand(10,10)\n",
    "        self.lr         = lr\n",
    "        pass\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        self.layer1 = Sigmoid.f(np.dot(x, self.weights1))\n",
    "        self.layer2 = Sigmoid.f(np.dot(self.layer1, self.weights2))\n",
    "        self.output = np.dot(self.layer2, self.weights3)\n",
    "        return self.output\n",
    "\n",
    "    def backprop(self, x, cost):\n",
    "        s2 = np.dot(cost, self.weights3.T) * Sigmoid.d(self.layer2)\n",
    "        s1 = np.dot(s2, self.weights2.T) * Sigmoid.d(self.layer1)\n",
    "        \n",
    "        d_weights3 = np.dot(self.layer2.T, cost)\n",
    "        d_weights2 = np.dot(self.layer1.T, s2)\n",
    "        d_weights1 = np.dot(x.T, s1)\n",
    "        \n",
    "        print(self.weights3)\n",
    "        \n",
    "        self.weights3 += d_weights3 * self.lr\n",
    "        self.weights2 += d_weights2 * self.lr\n",
    "        self.weights1 += d_weights1 * self.lr\n",
    "        pass\n",
    "\n",
    "nn2 = NeuralNetwork2(0.0005)\n",
    "epoch = 0\n",
    "target.clear()\n",
    "pred.clear()\n",
    "for X, T, e, step in gen_data(train, 1, 50):\n",
    "    Y = nn2.feedforward(X)\n",
    "    cost = SOS.d(T,Y)\n",
    "    nn2.backprop(X, cost)\n",
    "    if e > epoch:\n",
    "        epoch = e\n",
    "        loss = 0\n",
    "        nn2.lr -= nn2.lr * 0.001\n",
    "        for X, T, e, step in gen_data(val, 1, 1):\n",
    "            Y = nn2.feedforward(X)\n",
    "            loss += RMS.f(T, Y) / val.shape[0]\n",
    "        target.append(T)\n",
    "        pred.append(Y)\n",
    "        \n",
    "plt.plot(range(N), pred, label='prediction')\n",
    "plt.plot(range(N), target, label='target', linestyle=':')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':1, 'b':2}\n",
    "for k in a.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
